{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_6.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ1iNAqu6o9V"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
        "- `news_train.txt` тренировочное множество\n",
        "- `news_test.txt` тренировочное множество\n",
        "\n",
        "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
        "\n",
        "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
        "\n",
        ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Wv7kxe6o9Z"
      },
      "source": [
        "# Задача\n",
        "\n",
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "    \n",
        "    \n",
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
        "\n",
        "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия\n",
        "    \n",
        "\n",
        "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucZlZYtY6o9c"
      },
      "source": [
        "'https://drive.google.com/file/d/1mG3tPS_59pANrgwd6T2IgnHWgph4vYbg/view?usp=sharing'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-NQqj56o9d"
      },
      "source": [
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKrFZQ477qnE",
        "outputId": "fff9a6db-eb44-4f0d-a14f-b4d315180b7d"
      },
      "source": [
        "!pip install sentencepiece\r\n",
        "!pip install pymorphy2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 12.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 15.8MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 10.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 9.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 4.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 4.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: dawg-python, pymorphy2-dicts-ru, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsb_dUs96o9e"
      },
      "source": [
        "import re\n",
        "import sentencepiece as spm\n",
        "\n",
        "import smart_open as sm\n",
        "import gensim\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "import pymorphy2\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJHlxwW76o9f"
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMiPhIN6o9f"
      },
      "source": [
        "def read_data(f_in):\n",
        "    with sm.open(f_in, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            category, headline, text = line.strip().split('\\t')\n",
        "            yield category, headline, text\n",
        "            \n",
        "        \n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    \n",
        "    return words\n",
        "\n",
        "\n",
        "def prepare_spm_file(f_in):\n",
        "    data =[]\n",
        "    for category, headline, text in read_data(f_in):\n",
        "        \n",
        "        category = morph.parse(category)[0].normal_form\n",
        "        headline = [morph.parse(word)[0].normal_form for word in normalize_text(headline)]\n",
        "        text = [normalize_text(sent) for sent in re.split(r'[.!?]', text) if len(sent) > 50]\n",
        "        text_2 =[]\n",
        "        \n",
        "        for j in range(len(text)):\n",
        "            text_2.append([morph.parse(word)[0].normal_form for word in text[j]])\n",
        "        data.append([category, headline, text_2])\n",
        "       \n",
        "    return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spZE5KD86o9g"
      },
      "source": [
        "data_train = prepare_spm_file('./news_train.txt')\n",
        "data_test = prepare_spm_file('./news_test.txt')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnM8-iRs6o9h",
        "outputId": "d654a6cc-c72b-4d5f-de15-315da9e12441"
      },
      "source": [
        "print(data_train[0])\n",
        "print(data_test[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sport', ['овечкин', 'пожертвовать', 'детский', 'хоккейный', 'школа', 'автомобиль'], [['нападать', 'вашингтон', 'кэпиталзти', 'александр', 'овечкин', 'передать', 'детский', 'хоккейный', 'школа', 'автомобиль', 'получить', 'они', 'после', 'окончание', 'матч', 'весь', 'звезда', 'национальный', 'хоккейный', 'лига', 'нхл'], ['автомобиль', 'honda', 'accord', 'быть', 'подарить', 'хоккеист', 'по', 'решение', 'спонсор', 'мероприятие'], ['игрок', 'нхл', 'пожертвовать', 'машина', 'спортивный', 'школа', 'nova', 'cool', 'cats', 'special', 'hockey', 'inc'], ['овечкин', 'общаться', 'с', '10', 'летний', 'девочка', 'анна', 'чтоб', 'с', 'синдром', 'даун', 'который', 'заниматься', 'в', 'этот', 'школа', 'и', 'являться', 'поклонница', 'спортсмен'], ['в', 'сентябрь', 'форвард', 'пообедать', 'вместе', 'с', 'юный', 'хоккеистка', 'в', 'японский', 'ресторан'], ['матч', 'весь', 'звезда', 'нхл', 'в', 'коламбус', 'штат', 'огайо', 'завершиться', 'победа', 'команда', 'джонатан', 'тэйвз', 'над', 'команда', 'ник', 'фолиньо', 'с', 'счёт', '17', '12'], ['россиянин', 'отметиться', 'три', 'результативный', 'передача']]]\n",
            "['culture', ['жительница', 'ямал', 'победить', 'в', 'первый', 'песенный', 'конкурс', 'новый', 'звезда'], [['жительница', 'ямало', 'ненецкий', 'автономный', 'округ', 'елена', 'лаптандер', 'победить', 'в', 'первый', 'всероссийский', 'песенный', 'конкурс', 'новый', 'звезда', 'сообщить', 'лента'], ['в', 'качество', 'награда', 'она', 'достаться', 'статуэтка', 'в', 'форма', 'звезда', 'и', 'денежный', 'приз', 'один', 'миллион', 'рубль'], ['по', 'слово', 'финалистка', 'вознаграждение', 'она', 'планировать', 'передать', 'в', 'благотворительный', 'фонд', 'подарить', 'жизнь'], ['три', 'дополнительный', 'приз', 'достаться', 'руслана', 'ивакин', 'из', 'хакасия', 'фолкнуть', 'группа', 'ярил', 'зной', 'из', 'воронежский', 'область', 'и', 'александр', 'куулар', 'из', 'тыва'], ['призёр', 'с', 'помощь', 'смс', 'голосование', 'выбирать', 'зритель', 'телеканал', 'звезда', 'который', 'транслировать', 'конкурс'], ['весь', 'на', 'финальный', 'гала', 'концерт', 'выступить', '16', 'участник'], ['на', 'концерт', 'прозвучать', 'популярный', 'песня', 'военный', 'год', 'журавль', 'довоенный', 'вальс', 'мой', 'милый', 'если', 'б', 'не', 'быть', 'война', 'цветок', 'на', 'дорога', 'война', 'песня', 'десантный', 'штурмовой', 'батальон', 'из', 'кинофильм', 'белорусский', 'вокзал', 'и', 'другой'], ['на', 'это', 'конкурс', 'произойти', 'прорыв', 'наступление', 'фольклор'], ['отметить', 'композитор', 'максим', 'дунаевский', 'тоже', 'принять', 'участие', 'в', 'гала', 'концерт'], ['всероссийский', 'конкурс', 'исполнитель', 'песня', 'новый', 'звезда', 'проходить', 'при', 'поддержка', 'министерство', 'оборона', 'россия'], ['он', 'идея', 'принадлежать', 'глава', 'ведомство', 'сергей', 'шойгу'], ['благодаря', 'этот', 'проект', 'мы', 'услышать', 'не', 'тот', 'синтетик', 'который', 'сейчас', 'поголовно', 'петь', 'молодёжь', 'а', 'хороший', 'голос', 'и', 'настоящий', 'музыка'], ['надеяться', 'зритель', 'по', 'достоинство', 'оценить', 'тот', 'певец', 'который', 'быть', 'для', 'страна', 'новый', 'звезда', 'прокомментировать', 'шойгу']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuIvHqaE6o9i"
      },
      "source": [
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать gensim . Продемонстрировать семантические ассоциации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GopOjSq6o9j"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMVEhGmq6o9k"
      },
      "source": [
        "sentences = [word[1] for word in data_train]\n",
        "\n",
        "for sentence in data_train:\n",
        "    for i in range(len(sentence[2])):\n",
        "        sentences.extend([sentence[2][i]])\n",
        "\n",
        "w2v = Word2Vec(sentences, workers=8)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_mZZTtK6o9k",
        "outputId": "536aaeca-cf9a-43e2-c4b5-902ab5b30a5c"
      },
      "source": [
        "print(w2v.wv.most_similar(positive=[\"футбол\"]))\n",
        "print(w2v.wv.most_similar(positive=[\"искусство\"]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('хоккей', 0.9226015210151672), ('теннис', 0.8386198282241821), ('биатлон', 0.7951208353042603), ('фигурный', 0.7372260093688965), ('уефа', 0.7363604307174683), ('шахматы', 0.7299032807350159), ('баскетбол', 0.7280426025390625), ('кхл', 0.7242833375930786), ('чемпионка', 0.7230687141418457), ('кубок', 0.722800612449646)]\n",
            "[('театральный', 0.8301874399185181), ('литература', 0.7988033294677734), ('литературный', 0.768280565738678), ('наследие', 0.7552648782730103), ('произведение', 0.7485151290893555), ('школа', 0.7300301790237427), ('наука', 0.7288336157798767), ('колледж', 0.7242863178253174), ('изобразительный', 0.7233369946479797), ('современный', 0.7229183912277222)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyP3VHYj6o9l"
      },
      "source": [
        "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5bU2Wkp6o9m"
      },
      "source": [
        "max_item_len = 140"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97AKhrmy6o9p"
      },
      "source": [
        "dict_idx = {}\n",
        "def prepere_tfidf(data):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for sentences in data:\n",
        "        category, headline, text= sentences[0], sentences[1], sentences[2]\n",
        "\n",
        "        category_idx = dict_idx.get(category, len(dict_idx))\n",
        "        dict_idx[category] = category_idx\n",
        "        \n",
        "        word_idx = 0\n",
        "        sentence_idx = 0\n",
        "        pos_in_sent = 0\n",
        "\n",
        "        x = []\n",
        "\n",
        "        while word_idx < max_item_len:\n",
        "            if word_idx < len(headline):\n",
        "                x.append(headline[word_idx])\n",
        "                word_idx += 1\n",
        "            else:\n",
        "                if len(text)!=0 and pos_in_sent < len(text[sentence_idx]):\n",
        "                    x.append(text[sentence_idx][pos_in_sent])\n",
        "                    word_idx += 1\n",
        "                    pos_in_sent += 1\n",
        "                elif sentence_idx < len(text) - 1:\n",
        "                    sentence_idx += 1\n",
        "                    pos_in_sent = 0\n",
        "                else:\n",
        "                    x.append(\"tmp\")\n",
        "                    word_idx += 1\n",
        "\n",
        "        X.append(\" \".join(x))\n",
        "        y.append(category_idx)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "x_train_idf, y_train = prepere_tfidf(data_train)\n",
        "x_test_idf, y_test = prepere_tfidf(data_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY_H8RaH6o9q"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rb38j0E6o9r"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "x_train_idf = tfidf.fit_transform(x_train_idf)\n",
        "x_test_idf = tfidf.transform(x_test_idf)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1Rz7uFh6o9s",
        "outputId": "69b69607-4d67-4e9b-d380-9b8f9dc8149c"
      },
      "source": [
        "x_train_idf.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 75116)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaKfvdxEZ-oM"
      },
      "source": [
        "ЛОгистическая регрессия"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxaNWSdr6o9s",
        "outputId": "fbcaf203-37fd-407f-cde2-2edbc77131b6"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(n_jobs=-1)\n",
        "clf.fit(x_train_idf, y_train)\n",
        "preds = clf.predict(x_test_idf)\n",
        "print(\"точность: \" , (y_test == preds).mean())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "точность:  0.8583333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPm9DSKXZujD"
      },
      "source": [
        "подбор гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnyM_K2NWFFk",
        "outputId": "d5943059-8f5e-49c2-d31a-8e359c9b0434"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "hyper = {'C' : [1, 4, 10, 100], 'solver': ['lbfgs']}\r\n",
        "gd = GridSearchCV(estimator=\r\n",
        "                LogisticRegression(multi_class='multinomial', random_state=17, n_jobs=4),\r\n",
        "                param_grid=hyper)\r\n",
        "gd.fit(x_train_idf, y_train)\r\n",
        "print(gd.best_score_)\r\n",
        "print(gd.best_estimator_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8612666666666666\n",
            "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='multinomial', n_jobs=4, penalty='l2',\n",
            "                   random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWAwfBfbWg8d",
        "outputId": "0403eaa9-dc1c-4bf5-e1f5-04b46c0f27b9"
      },
      "source": [
        "clf_lr = gd.best_estimator_\r\n",
        "clf_lr.fit(x_train_idf, y_train)\r\n",
        "preds = clf_lr.predict(x_test_idf)\r\n",
        "print(f\"accuracy = {(y_test == preds).mean()}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy = 0.8743333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCBaYOOGaIeZ"
      },
      "source": [
        "наивный байесовский классификатор"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0cMB_mD6o9t",
        "outputId": "03c7ef0c-6226-4e24-bd61-1dddef2050c2"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x_train_idf, y_train)\n",
        "preds = clf.predict(x_test_idf)\n",
        "print(\"точность: \" , (y_test == preds).mean())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "точность:  0.798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl4zqES56o9t"
      },
      "source": [
        "(гиперпараметры отсутствуют)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH7ky2t46o9u"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}